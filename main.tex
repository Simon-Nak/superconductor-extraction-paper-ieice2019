\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{tabularx}
\usepackage{url}

\title{Proposal of Automatic Extraction Framework of Superconductors related Information from Scientific literature}

\author[1]{Luca Foppiano\thanks{FOPPIANO.Luca@nims.go.jp}}
\author[1]{Thaer M. Dieb\thanks{MOUSTAFADIEB.Thaer@nims.go.jp}}
\author[1]{Akira Suzuki\thanks{SUZUKI.Akira3@nims.go.jp}}
\author[1]{Masashi Ishii\thanks{ISHII.Masashi@nims.go.jp}}
\affil[1]{Research and Services Division of Materials Data and Integrated System (MaDIS), National Institute for Materials Science (NIMS), 1-2-1 Sengen, Tsukuba, Ibaraki 305-0047, Japan}

% \date{April 2019}

\begin{document}

\maketitle

\begin{abstract}
Automatic collection of materials information from research papers using natural language processing is highly required for rapid materials development using big data, namely materials informatics (MI). Difficulty of this automatic collection is mainly caused by the variety of expressions in the papers, a device with tolerance to such variety is required to be developed. 
In this paper, we report an ongoing interdisciplinary work to construct the device for automatic collection of superconductor-related information from scientific literature using text mining techniques. We focused on identification of superconducting materials and their key property of critical temperature (Tc), and discussed machine learning (ML) techniques, including annotation strategies to obtain appropriate training data. We introduce a guideline for the annotation together with our several trails of ML on subsequent automatic data collection.
\end{abstract}

%% The table of content is there just for organisation purposes, will be removed 
\pagebreak

\tableofcontents

\pagebreak

%Research in superconductors is always articulated over two main axes, finding new conditions or discovering new materials (or combination of it) show new or better superconductivity properties. 
%In order to do so, material scientists needs to have rapid access to materials known to be superconductors and their properties without have to examine the thousand of papers related to it. Such data can also be used by further systems to compute generative models 


\section{Introduction}
% What is the problem we are trying to solve? What are the motivation behind this project? 

Automatic extraction of information from research papers using Natural Language processing is a highly required approach in many domains. In material research, the availability of large quantity of experimental data can give hints and ideas leading to new break-trough materials discoveries. Large availability of scientific papers and the expertise costs for manually generated such data justify the needs of Text and Data Mining automatic approaches.

In this paper we describe the ongoing attempt to design a system aiming to automatically extract superconductor information from scientific literature based on Machine Learning, using Natural Language Processing techniques.

Writing style, variability in experiment and result description are just two of the variables making this task particularly complex. Reason why the best approach to successfully deliver a functioning system is to reduce the complexity to the smallest viable product.
We focus on extracting materials names with doping rates and critical temperature (Tc). We foreseen the use of probabilistic or neural models, our work begin designing the training data schema and define a set of guidelines \cite{article} 
In order to assess the feasibility of our approach, while producing a working prototype in short time, we based our design on an existing open source product: Grobid \cite{GROBID} \cite{lopez2009grobid} a Machine Learning library for extracting information from scholarly documents. We implemented a module specialised in superconductors papers.

As an interdisciplinary collaboration, this project we receive help and collaborate with the material scientists and engineers division. 

%Summary of the next sections - to be discussed
This article is divided as following. In Section \ref{sec:requirements} we describe the domain and the domain expert's requirements. In Section \ref{sec:overview} we will then describe the overall system design and the process of designing the schema for the ML training data. 

\section{The Superconductor research domain}
\label{sec:requirements}
%% How research is made and what are the point of improvements

% from wikipedia - begin 
\textit{Superconductivity is a phenomenon of exactly zero electrical resistance and expulsion of magnetic flux fields occurring in certain materials, called superconductors, when cooled below a characteristic critical temperature.}\footnote{\url{https://en.wikipedia.org/wiki/Superconductivity}}
% end

The research in superconductor materials is articulated toward many different objectives. Discovery of new characteristic of well known materials, under new environment condition, like applied pressure or magnetic field. Combination of known superconductors with non-superconductors may lead to new materials with better characteristics, usually a higher critical temperature. A superconductor scientist accessing a large detailed database with extracted properties from materials could find new leads toward his research, like designing new experiments with different conditions. 

% Should we need to mention supercon? Maybe not 
NIMS database SuperCon\cite{SuperCon} it's a manually curated database containing about 32k inorganic superconductors definitions, discovered in papers. 
The goal of this project is to find a reliable way to generate such database automatically. 

\subsection{Information of interest}
As part of the usual step of assessing the feasibility, and collecting requirements from domain expert, we briefly describe information that domain expert ideally expects to be available in a automatically generated knowledge base for superconductors. 

\begin{center}
    \begin{tabular}{ | m{5em} | m{8cm}| } 
    \hline
        Name & Description  \\ [0.5ex] 
    \hline\hline
        T\textsubscript{c} & Critical Temperature\\ 
    \hline
        T\textsubscript{onset}, T\textsubscript{offset} & Temperature where the resistance tend to zero (offset) to when is really zero (onset)\\ 
    \hline
        H\textsubscript{c} & Critical field\\ 
    \hline
        I\textsubscript{c} & Critical current\\
    \hline
        H\textsubscript{c} & Critical field\\ 
    \hline
        J\textsubscript{c} & Critical current density\\ 
    \hline
        H\textsubscript{ivr} & Irreversibly field\\
    \hline
        Crystal structure space group & \\
    \hline
        Sample preparation shape & single crystal, poly crystal, thin film or wire. \\
    \hline    
    \end{tabular}
\end{center}

The representation of such information is not straightforward because they can be described in plots, tables as well as plain text. Plots of importance are usually showing Tc against other experimental parameters (doping level, x, pressure, etc.) or magnetisation. 

\section{System design}
The design of the system is a continuous iterative process. In this section we describe the design actualised at the current state of the project, expecting some evolution and adaptation as the problem become more clear. 
The problem can be divided into two phases: the extraction of relevant entities and the linking or clustering of such entities according certain criteria. 
While the first phase is a common example of sequence Labelling, the linking needs to be more tight coupled with the type of data to link. 

There are basic principles we are following to ensure rapid prototyping and avoiding to reinvent the wheel. We try to focus on the specificity of the problem in superconductor rather than creating yet another sequence labelling system from scratch. For this reason we based approach on Grobid\cite{GROBID}: for the recognition of the property values, we have reused a Grobid-based model specialised in measurement extraction called \textit{grobid-quantitites}\cite{grobid-quantities}, shipping already good results in recognising measurements, like temperature, pressure and providing parsing and normalisation. For the other information we have created a new module specialised in superconductor data. Moreover Grobid provides an API and web interface allowing the visualisation of the annotation on-the-fly on top of the PDF documents. 

\subsection{Engine}
\label{sec:overview}
The system is based on Machine Learning techniques, currently using CRF (Conditional Random Field) for practical reasons. We plan to test the system using Bi-LSTM combined with CRF once the amount of training data are enough to provide an reliable comparison. 
For the linking phase, we have currently implemented an heuristic approach to 1) tag any mention to a temperature as critical temperature and 2) link any critical temperature to a material whose it is referring to. 

\subsection{Training data}
% In this section we are discussing the process of annotation, in particular: 
% 1. how did we find the right balance of annotation, 
% 2.  results in term of IAA

Machine Learning bring many advantages in term of accuracy and precision [add ref], tolerance with noise [add references] and flexibility in recognising entities that have never been seen before. 
Unfortunately ML-based systems require training data which is often not available in great quantity. 
The process of annotation of new training data is very expensive: 
\begin{enumerate}
    \item greedy in term of time and resources
    \item the system might require a lot of data before showing performance accuracy improvements
    \item is a tedious and frustrating for annotators (usually domain experts,  feeling overly-skilled, thus less motivated)
    \item throughput and precision are inversely proportional
\end{enumerate}

Training data are written as a collaborative work between domain experts and engineers. Before involving the domain experts, however a reasonable approach is to agree a common understanding "internally" among engineers and data scientists that are actively designing the machine learning system. 

This approach bring two advantages: 
\begin{itemize}
    \item assessment and knowledge of the domain, 
    \item it is likely to hit more problems than what imagined by actively performing the task 
    \item early issues help to think more proactively about possible shortcut, solution or additional constraints, and 
    \item we would be better prepared to justify eventual decision to the domain experts
\end{itemize}

The outcome is then discussed and validated with domain experts. 

% This section should describe how we are doing and which problems we are facing
\section{Ongoing Results}
\subsection{Prototype status}

\subsection{Annotation status}
% In this section we describe in detail the various iterations - with a table? 

% first iteration
As a first step we, engineers and data scientists, have annotated two sample documents, without any special constraints, to the best to our knowledge and then we have compared the results. 

the goal was to get a first understanding of the papers and exchange our unique view about the task to be solved. 

here the result...


What has emerged were the following points: 

\section{Conclusions}
% ML 
In the sequence labelling phase we verified that CRF can perform pretty well even without any particular information. The features provided by Chemspot contributed only for 1\% in term of F-1score. On the other hand, Chemspot increased the response time of a PDF document by 10 times, making the system not particularly suitable for processing of large collection of data. 

% Linking
The Heuristic linking has shown many lack and very low tolerance to noise, we foreseen to improve it by 1) improve the current appraoch adding more rules 2)  implement a simple CRF model that allows the linking of relatively closed entities, and 3) study the possibility to exploit a sentence dependency parsing, bearing in mind the impact in performances should also be considered.


\listoffigures

\bibliography{references}
\bibliographystyle{plain}

\end{document}
